import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

print("=" * 60)
print("ğŸ¯ BÃ€I TOÃN PHÃ‚N LOáº I STRESS SINH VIÃŠN Vá»šI PCA")
print("=" * 60)

# =============================================================================
# BÆ¯á»šC 1: THU THáº¬P VÃ€ MÃ” Táº¢ Dá»® LIá»†U
# =============================================================================
print("\nğŸ” BÆ¯á»šC 1: ÄANG Táº¢I VÃ€ PHÃ‚N TÃCH Dá»® LIá»†U...")

# Load dataset
df = pd.read_excel('dataset.xlsx')

print("ğŸ“Š THÃ”NG TIN DATASET:")
print(f"- Sá»‘ lÆ°á»£ng máº«u: {df.shape[0]}")
print(f"- Sá»‘ lÆ°á»£ng Ä‘áº·c trÆ°ng: {df.shape[1]}")
print(f"- CÃ¡c Ä‘áº·c trÆ°ng: {list(df.columns)}")

# Kiá»ƒm tra phÃ¢n phá»‘i target
print(f"\nğŸ“ˆ PHÃ‚N PHá»I STRESS_LEVEL:")
stress_distribution = df['stress_level'].value_counts().sort_index()
print(stress_distribution)

# Hiá»ƒn thá»‹ Ã½ nghÄ©a cÃ¡c má»©c Ä‘á»™ stress
stress_mapping = {0: "Tháº¥p (Low)", 1: "Trung bÃ¬nh (Moderate)", 2: "Cao (High)"}
for level, count in stress_distribution.items():
    print(f"  - Level {level} ({stress_mapping[level]}): {count} máº«u ({count/len(df)*100:.1f}%)")

# =============================================================================
# BÆ¯á»šC 2: TIá»€N Xá»¬ LÃ Dá»® LIá»†U
# =============================================================================
print("\n" + "=" * 50)
print("âš™ï¸ BÆ¯á»šC 2: TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
print("=" * 50)

# 2.1. Kiá»ƒm tra vÃ  xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u
print("\nğŸ” KIá»‚M TRA GIÃ TRá»Š THIáº¾U:")
missing_values = df.isnull().sum()
print(missing_values[missing_values > 0])

if missing_values.sum() > 0:
    print("ğŸ”„ Äang xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u...")
    # Thay tháº¿ giÃ¡ trá»‹ thiáº¿u báº±ng median cho numerical features
    imputer = SimpleImputer(strategy='median')
    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)
    print("âœ… ÄÃ£ xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u")
else:
    print("âœ… KhÃ´ng cÃ³ giÃ¡ trá»‹ thiáº¿u")
    df_imputed = df.copy()

# 2.2. Kiá»ƒm tra vÃ  mÃ£ hÃ³a nhÃ£n phÃ¢n loáº¡i
print("\nğŸ”  KIá»‚M TRA MÃƒ HÃ“A NHÃƒN:")
print(f"- GiÃ¡ trá»‹ duy nháº¥t trong stress_level: {sorted(df_imputed['stress_level'].unique())}")
print(f"- Kiá»ƒu dá»¯ liá»‡u: {df_imputed['stress_level'].dtype}")

# Kiá»ƒm tra xem nhÃ£n Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a sá»‘ chÆ°a
if df_imputed['stress_level'].dtype == 'object':
    print("ğŸ”„ Äang mÃ£ hÃ³a nhÃ£n phÃ¢n loáº¡i...")
    label_mapping = {'Low': 0, 'Moderate': 1, 'High': 2}
    df_imputed['stress_level'] = df_imputed['stress_level'].map(label_mapping)
    print("âœ… ÄÃ£ mÃ£ hÃ³a nhÃ£n phÃ¢n loáº¡i")
else:
    print("âœ… NhÃ£n Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a sá»‘")

# 2.3. Chuáº©n bá»‹ features vÃ  target
X = df_imputed.drop(columns=['stress_level'])
y = df_imputed['stress_level']

print(f"\nğŸ“‹ THÃ”NG TIN SAU TIá»€N Xá»¬ LÃ:")
print(f"- Features: {X.shape[1]} Ä‘áº·c trÆ°ng")
print(f"- Target: {len(np.unique(y))} classes")

# 2.4. Chuáº©n hÃ³a dá»¯ liá»‡u
print("\nğŸ“ CHUáº¨N HÃ“A Dá»® LIá»†U...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("âœ… ÄÃ£ chuáº©n hÃ³a dá»¯ liá»‡u vá»›i StandardScaler")

# 2.5. Feature Extraction vá»›i PCA - PC1 vÃ  PC2
print("\nğŸ”§ FEATURE EXTRACTION Vá»šI PCA (PC1, PC2)...")
pca = PCA(n_components=2)  # Chá»‰ láº¥y 2 components chÃ­nh
X_pca = pca.fit_transform(X_scaled)

print("âœ… ÄÃ£ thá»±c hiá»‡n PCA:")
print(f"- Sá»‘ components: {pca.n_components_}")
print(f"- PhÆ°Æ¡ng sai Ä‘Æ°á»£c giá»¯: {pca.explained_variance_ratio_.sum():.3f}")
print(f"- PhÆ°Æ¡ng sai PC1: {pca.explained_variance_ratio_[0]:.3f}")
print(f"- PhÆ°Æ¡ng sai PC2: {pca.explained_variance_ratio_[1]:.3f}")
print(f"- KÃ­ch thÆ°á»›c dá»¯ liá»‡u sau PCA: {X_pca.shape}")

# Hiá»ƒn thá»‹ cÃ¡c features Ä‘Ã³ng gÃ³p vÃ o PC1 vÃ  PC2
print("\nğŸ“Š PHÃ‚N TÃCH COMPONENTS:")
feature_names = X.columns
pca_components_df = pd.DataFrame({
    'Feature': feature_names,
    'PC1': pca.components_[0],
    'PC2': pca.components_[1],
    'PC1_Abs': np.abs(pca.components_[0]),
    'PC2_Abs': np.abs(pca.components_[1])
})

print("\nğŸ” TOP 5 FEATURES QUAN TRá»ŒNG CHO PC1:")
print(pca_components_df.nlargest(5, 'PC1_Abs')[['Feature', 'PC1']].to_string(index=False))

print("\nğŸ” TOP 5 FEATURES QUAN TRá»ŒNG CHO PC2:")
print(pca_components_df.nlargest(5, 'PC2_Abs')[['Feature', 'PC2']].to_string(index=False))

# =============================================================================
# BÆ¯á»šC 3: HUáº¤N LUYá»†N MÃ” HÃŒNH
# =============================================================================
print("\n" + "=" * 50)
print("ğŸ¤– BÆ¯á»šC 3: HUáº¤N LUYá»†N MÃ” HÃŒNH")
print("=" * 50)

print("\nğŸ“Š CHIA Dá»® LIá»†U TRAIN/TEST...")

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)


print(f"- Táº­p train (gá»‘c): {X_train.shape[0]} máº«u, {X_train.shape[1]} features")
print(f"- Táº­p test (gá»‘c): {X_test.shape[0]} máº«u, {X_test.shape[1]} features")
print("âœ… ÄÃ£ chia dá»¯ liá»‡u")

print("\nğŸ¯ HUáº¤N LUYá»†N MÃ” HÃŒNH KNN...")

param_grid = {
    'n_neighbors': range(3, 16, 2),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

print("ğŸ” TÃŒM THAM Sá» Tá»I Æ¯U CHO Dá»® LIá»†U Gá»C...")
knn = KNeighborsClassifier()
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)



best_knn = grid_search.best_estimator_

print(f"\nâœ… THAM Sá» Tá»I Æ¯U : {grid_search.best_params_}")

# =============================================================================
## =============================================================================
# BÆ¯á»šC 4: ÄÃNH GIÃ
# =============================================================================

print("\n" + "=" * 50)
print("ğŸ“Š BÆ¯á»šC 4: ÄÃNH GIÃ MÃ” HÃŒNH KNN")
print("=" * 50)

# Dá»± Ä‘oÃ¡n trÃªn táº­p test
y_pred = best_knn.predict(X_test)

# TÃ­nh Ä‘á»™ chÃ­nh xÃ¡c
test_accuracy = accuracy_score(y_test, y_pred)

print("ğŸ“ˆ Káº¾T QUáº¢ ÄÃNH GIÃ MÃ” HÃŒNH KNN:")
print(f"ğŸ”¹ Äá»™ chÃ­nh xÃ¡c (Accuracy): {test_accuracy:.4f}")

print("\nğŸ“‹ BÃO CÃO CHI TIáº¾T:")
print(classification_report(y_test, y_pred, target_names=['Tháº¥p', 'Trung bÃ¬nh', 'Cao']))


# =============================================================================
# TRá»°C QUAN HÃ“A â€“ XÃ“A HOÃ€N TOÃ€N Tá»¶ Lá»† PHÆ¯Æ NG SAI
# =============================================================================

print("\nğŸ¨ Váº¼ BIá»‚U Äá»’ PCA...")

plt.figure(figsize=(15, 10))

# ============================================
# 1ï¸âƒ£ SCATTER PLOT PCA
# ============================================
plt.subplot(2, 2, 1)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
plt.colorbar(scatter, label='Má»©c Ä‘á»™ Stress')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PhÃ¢n bá»‘ dá»¯ liá»‡u trÃªn PC1 vÃ  PC2')
plt.grid(True, alpha=0.3)

# ============================================
# 2ï¸âƒ£ BIá»‚U Äá»’ ÄÃ“NG GÃ“P THÃ€NH PHáº¦N
# ============================================
plt.subplot(2, 2, 2)
top_features_pc1 = pca_components_df.nlargest(8, 'PC1_Abs')

sns.heatmap(
    top_features_pc1[['PC1', 'PC2']],
    annot=True,
    cmap='coolwarm',
    center=0,
    yticklabels=top_features_pc1['Feature']
)
plt.title('Top Features cho PC1 & PC2')

# ============================================
# 3ï¸âƒ£ CONFUSION MATRIX (KNN Gá»C)
# ============================================
plt.subplot(2, 2, 3)
cm = confusion_matrix(y_test, y_pred)

sns.heatmap(
    cm, annot=True, fmt='d', cmap='Blues',
    xticklabels=['Tháº¥p', 'Trung bÃ¬nh', 'Cao'],
    yticklabels=['Tháº¥p', 'Trung bÃ¬nh', 'Cao']
)

plt.title('Ma tráº­n nháº§m láº«n (KNN Gá»‘c)')
plt.xlabel('Dá»± Ä‘oÃ¡n')
plt.ylabel('Thá»±c táº¿')

plt.tight_layout()
plt.savefig('pca_analysis_results.png', dpi=300, bbox_inches='tight')
plt.show()


# =============================================================================
# BÆ¯á»šC 5: LÆ¯U MÃ” HÃŒNH
# =============================================================================
print("\n" + "=" * 50)
print("ğŸ’¾ LÆ¯U MÃ” HÃŒNH")
print("=" * 50)

# LÆ°u mÃ´ hÃ¬nh KNN gá»‘c
joblib.dump(best_knn, 'stress_knn_model.pkl')

# LÆ°u scaler
joblib.dump(scaler, 'scaler.pkl')

# LÆ°u tham sá»‘ tá»‘t nháº¥t
joblib.dump(grid_search.best_params_, 'best_params.pkl')

print("ğŸ’¾ ÄÃ£ lÆ°u stress_knn_model.pkl, scaler.pkl, best_params.pkl")
print("=" * 60)
print("ğŸ¯ HOÃ€N THÃ€NH â€“ KHÃ”NG Sá»¬ Dá»¤NG PCA CHO MÃ” HÃŒNH")
print("=" * 60)
